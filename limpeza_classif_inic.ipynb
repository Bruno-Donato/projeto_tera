{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas e dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import pt_core_news_lg\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55199\\AppData\\Local\\Temp\\ipykernel_18580\\1077869811.py:1: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/erika_hilton.csv', on_bad_lines='skip', sep=';', encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/erika_hilton.csv', on_bad_lines='skip', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22505, 49)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrar tweets?\n",
    "Pensei em filtrar os tweets, deixando somente os tweets direcionados a Erika, e para remover duplicatas.\n",
    "Nessa fase não apliquei somente pra termos mais tweets para análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usuarios = df[df['user'] != \"https://twitter.com/ErikakHilton\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19390, 49)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usuarios.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo \"não\" das stopwords\n",
    "STOP_WORDS.remove('não')\n",
    "\n",
    "# Instanciando spacy/pt_core_news_lg - corpus completo\n",
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doze', 'fazeis', 'e', 'depois', 'antes', 'com', 'pode', 'último', 'três', 'lá', 'menor', 'tipo', 'porquanto', 'todo', 'final', 'parece', 'ora', 'vem', 'faz', 'segundo', 'ser', 'outras', 'isso', 'fora', 'disso', 'estivestes', 'pôde', 'terceiro', 'vezes', 'aquelas', 'sexta', 'em', 'portanto', 'tens', 'forma', 'geral', 'contudo', 'vos', 'pouca', 'sabe', 'treze', 'apoia', 'à', 'quarta', 'dentro', 'poderá', 'comprida', 'usa', 'vais', 'no', 'seis', 'fazes', 'aí', 'local', 'vão', 'cujo', 'primeiro', 'comprido', 'devem', 'estes', 'muito', 'boa', 'nove', 'quem', 'outros', 'muitos', 'vós', 'sem', 'quinta', 'pegar', 'apenas', 'zero', 'pelo', 'além', 'agora', 'talvez', 'acerca', 'daquela', 'teu', 'caminho', 'porquê', 'cada', 'quatro', 'seus', 'tanta', 'quanto', 'após', 'ir', 'adeus', 'grande', 'mais', 'dez', 'te', 'esses', 'num', 'novos', 'tudo', 'suas', 'perto', 'dezasseis', 'numa', 'onze', 'elas', 'quero', 'tivemos', 'veja', 'estiveram', 'ou', 'nosso', 'partir', 'logo', 'os', 'possivelmente', 'esteve', 'já', 'vens', 'quer', 'diz', 'dizem', 'vinda', 'neste', 'nesse', 'só', 'foste', 'segunda', 'querem', 'coisa', 'dezanove', 'enquanto', 'até', 'tivestes', 'todas', 'oitavo', 'custa', 'vindo', 'estado', 'estou', 'conhecida', 'meus', 'nas', 'entre', 'sei', 'inicio', 'dezoito', 'de', 'sou', 'nesta', 'seu', 'foi', 'próximo', 'sexto', 'grupo', 'isto', 'este', 'tua', 'sete', 'são', 'às', 'número', 'nunca', 'fomos', 'assim', 'menos', 'duas', 'põem', 'fazem', 'cuja', 'mês', 'vai', 'fazemos', 'grandes', 'és', 'mal', 'tanto', 'próxima', 'tentar', 'sobre', 'tais', 'vossos', 'novas', 'nessa', 'minha', 'alguns', 'pelos', 'ademais', 'questão', 'sempre', 'quinze', 'nuns', 'breve', 'eu', 'tal', 'quinto', 'cima', 'do', 'meses', 'certamente', 'as', 'dos', 'você', 'na', 'bastante', 'naquele', 'atrás', 'ambos', 'dezassete', 'posição', 'mas', 'fará', 'me', 'nada', 'aos', 'tentei', 'números', 'pontos', 'uma', 'lhe', 'for', 'vossa', 'através', 'desde', 'vez', 'ela', 'cento', 'cedo', 'área', 'nenhuma', 'esse', 'povo', 'daquele', 'estão', 'mesmo', 'fostes', 'podia', 'das', 'uns', 'vocês', 'nova', 'temos', 'relação', 'desse', 'para', 'longe', 'cinco', 'ambas', 'desta', 'se', 'quarto', 'tu', 'pois', 'tempo', 'debaixo', 'estiveste', 'momento', 'pelas', 'bem', 'contra', 'inclusive', 'sua', 'então', 'sim', 'diante', 'possível', 'qual', 'quais', 'quieta', 'põe', 'baixo', 'seria', 'está', 'embora', 'tendes', 'tiveste', 'terceira', 'lugar', 'aquela', 'como', 'fazer', 'deve', 'aqui', 'da', 'nem', 'faço', 'toda', 'usar', 'umas', 'poder', 'ainda', 'ponto', 'aquele', 'também', 'porém', 'máximo', 'posso', 'aquilo', 'sistema', 'tem', 'nível', 'parte', 'tentaram', 'somos', 'ele', 'nós', 'estará', 'foram', 'dois', 'teve', 'ali', 'naquela', 'estar', 'oito', 'eles', 'quieto', 'a', 'dizer', 'ver', 'algo', 'onde', 'tão', 'estas', 'conhecido', 'certeza', 'valor', 'pela', 'maioria', 'apoio', 'próprio', 'têm', 'maiorias', 'bom', 'teus', 'conselho', 'quando', 'vêm', 'sétima', 'falta', 'tenho', 'o', 'essas', 'tarde', 'direita', 'fazia', 'é', 'qualquer', 'novo', 'nos', 'estás', 'iniciar', 'ter', 'maior', 'favor', 'essa', 'fez', 'outra', 'minhas', 'vinte', 'meu', 'eventual', 'vosso', 'corrente', 'meio', 'estive', 'des', 'por', 'somente', 'oitava', 'que', 'saber', 'nossas', 'esta', 'quê', 'tive', 'deverá', 'tente', 'pouco', 'nossos', 'tiveram', 'vossas', 'dar', 'tuas', 'exemplo', 'demais', 'irá', 'obrigada', 'porque', 'deste', 'algumas', 'dão', 'estivemos', 'vários', 'estava', 'mil', 'ligado', 'dá', 'primeira', 'ao', 'nossa', 'ontem', 'puderam', 'sob', 'todos', 'cá', 'um', 'era', 'lado', 'aqueles', 'sétimo', 'sois', 'dessa', 'catorze', 'apontar', 'fui', 'obrigado', 'fim', 'podem'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_usuarios['rawContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11744                       @ErikakHilton @silviokiwi DIVA\n",
       "9395     @ErikakHilton É mais fácil o eleitor mudar de ...\n",
       "12798                @ErikakHilton https://t.co/Qd5cKP4qm2\n",
       "18809    @ErikakHilton @cortezpsol @GuilhermeBoulos @bf...\n",
       "20870    @ErikakHilton Tô indo mami. Agora é a hora \\n1...\n",
       "6699     @ErikakHilton É bom lembrar que se gastar mais...\n",
       "17650    @ErikakHilton @_danielaabade Ler os comentário...\n",
       "13622              @ErikakHilton é de muita HIPOCRISIA !!!\n",
       "3602                 @ErikakHilton #ForaBolsonaroMentiroso\n",
       "6051     @ErikakHilton PQP!!!!\\nPQP!!!!\\nPQP!!!!\\nPQP!!...\n",
       "Name: rawContent, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover URL\n",
    "def remover_url(texto):\n",
    "    texto = re.sub(r'http\\S+', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover contas\n",
    "def remover_twiters(texto):\n",
    "    texto = re.sub(r'@\\w+', '@MENÇÃO', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover pontuações\n",
    "def remover_pontuacao(texto):\n",
    "    texto = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demojizar(x):\n",
    "    x = emoji.demojize(x, language='pt')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando limpezas\n",
    "tweets = tweets.str.lower()\n",
    "tweets = tweets.apply(remover_url)\n",
    "tweets = tweets.apply(remover_twiters)\n",
    "tweets = tweets.apply(remover_pontuacao)\n",
    "tweets = tweets.apply(demojizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remoção de stopwords e digitos\n",
    "def limpar_texto(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_digit]\n",
    "    texto_limpo = \" \".join(tokens)\n",
    "    return texto_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.apply(limpar_texto)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentativa de corrigir typos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essa biblioteca seria possivel corrigir erros de digitação.<br>\n",
    "Em alguns tweets percebi que tinham palavras com letras repetidas ou contrações e isso poderia dificutar a nossa análise.<br>\n",
    "Mas não consegui aplicar essa etapa da limpeza. Vou deixar marcado caso alguem queira tentar aplicar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando spellchecker\n",
    "spell = SpellChecker(language='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para correção de grafia\n",
    "def corretor_grafia(texto):\n",
    "    tb = TextBlob(texto)\n",
    "    corrected_words = []\n",
    "    for word in tb.words:\n",
    "        corrected_word = word.correct()\n",
    "        corrected_words.append(corrected_word)\n",
    "    \n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        longo dia recebendo denúncias pessoas presas h...\n",
       "1          simplesmente : coração_vermelho::coração_ver...\n",
       "2          hahahhaha : coração_roxo::coração_roxo::cora...\n",
       "3        longo dia recebendo denúncias pessoas presas h...\n",
       "4                                                         \n",
       "                               ...                        \n",
       "22500          casa vc   opções mãe tias candidatas saí...\n",
       "22501                                             sairão ♡\n",
       "22502    candidatos pra estadual federal respectivamente  \n",
       "22503                         força fabio : coração_roxo :\n",
       "22504              ownnn conte comigo : coração_vermelho :\n",
       "Name: rawContent, Length: 22505, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets \u001b[39m=\u001b[39m tweets\u001b[39m.\u001b[39;49mapply(corretor_grafia)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mcorretor_grafia\u001b[1;34m(texto)\u001b[0m\n\u001b[0;32m      4\u001b[0m corrected_words \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tb\u001b[39m.\u001b[39mwords:\n\u001b[1;32m----> 6\u001b[0m     corrected_word \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39;49mcorrect()\n\u001b[0;32m      7\u001b[0m     corrected_words\u001b[39m.\u001b[39mappend(corrected_word)\n\u001b[0;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m corrected_words\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\blob.py:142\u001b[0m, in \u001b[0;36mWord.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m Word(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspellcheck()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\blob.py:134\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspellcheck\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    126\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[39m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m suggest(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\en\\__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msuggest\u001b[39m(w):\n\u001b[0;32m    121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m spelling\u001b[39m.\u001b[39;49msuggest(w)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:1399\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m w\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misdigit():\n\u001b[0;32m   1396\u001b[0m     \u001b[39mreturn\u001b[39;00m [(w, \u001b[39m1.0\u001b[39m)] \u001b[39m# 1.5\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known([w]) \\\n\u001b[0;32m   1398\u001b[0m           \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w)) \\\n\u001b[1;32m-> 1399\u001b[0m           \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit2(w)) \\\n\u001b[0;32m   1400\u001b[0m           \u001b[39mor\u001b[39;00m [w]\n\u001b[0;32m   1401\u001b[0m candidates \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(c, \u001b[39m0.0\u001b[39m), c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m candidates]\n\u001b[0;32m   1402\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39msum\u001b[39m(p \u001b[39mfor\u001b[39;00m p, word \u001b[39min\u001b[39;00m candidates) \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:1376\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39;49m(e2 \u001b[39mfor\u001b[39;49;00m e1 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit1(w) \u001b[39mfor\u001b[39;49;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit1(e1) \u001b[39mif\u001b[39;49;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:1376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(e2 \u001b[39mfor\u001b[39;00m e1 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w) \u001b[39mfor\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(e1) \u001b[39mif\u001b[39;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:96\u001b[0m, in \u001b[0;36mlazydict.__contains__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__contains__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy(\u001b[39m\"\u001b[39;49m\u001b[39m__contains__\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:84\u001b[0m, in \u001b[0;36mlazydict._lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lazy\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     81\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" If the dictionary is empty, calls lazydict.load().\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m        Replaces lazydict.method() with dict.method() and calls it.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mdict\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__len__\u001b[39;49m(\u001b[39mself\u001b[39;49m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     85\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[0;32m     86\u001b[0m         \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, method, types\u001b[39m.\u001b[39mMethodType(\u001b[39mgetattr\u001b[39m(\u001b[39mdict\u001b[39m, method), \u001b[39mself\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweets = tweets.apply(corretor_grafia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando em dataframe\n",
    "df_tweets = tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MENÇÃO simplesmente : coração_vermelho::coraçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENÇÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MENÇÃO : mão_beliscando_pele_clara::coração_ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MENÇÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MENÇÃO simplesmente : coração_vermelho::coraçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>ovotoésecreto \\n  deputada estadual MENÇÃO \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO casa vc MEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO sairão ♡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>MENÇÃO MENÇÃO ownnn conte comigo : coração_ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19390 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent\n",
       "1      MENÇÃO simplesmente : coração_vermelho::coraçã...\n",
       "4                                                 MENÇÃO\n",
       "6      MENÇÃO : mão_beliscando_pele_clara::coração_ve...\n",
       "10                                                MENÇÃO\n",
       "12     MENÇÃO simplesmente : coração_vermelho::coraçã...\n",
       "...                                                  ...\n",
       "22498  ovotoésecreto \\n  deputada estadual MENÇÃO \\n ...\n",
       "22500  MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO casa vc MEN...\n",
       "22501        MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO sairão ♡\n",
       "22502  candidatos pra estadual federal respectivament...\n",
       "22504  MENÇÃO MENÇÃO ownnn conte comigo : coração_ver...\n",
       "\n",
       "[19390 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando e excluindo linhas vazias. \n",
    "# Aqui não foi possível excluir nem identificar com funções NA, pq a linhas estava vazia.\n",
    "df_tweets = df_tweets[df_tweets['rawContent'] != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MENÇÃO simplesmente : coração_vermelho::coraçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENÇÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MENÇÃO : mão_beliscando_pele_clara::coração_ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MENÇÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MENÇÃO simplesmente : coração_vermelho::coraçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>ovotoésecreto \\n  deputada estadual MENÇÃO \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO casa vc MEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO sairão ♡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>MENÇÃO MENÇÃO ownnn conte comigo : coração_ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19390 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent\n",
       "1      MENÇÃO simplesmente : coração_vermelho::coraçã...\n",
       "4                                                 MENÇÃO\n",
       "6      MENÇÃO : mão_beliscando_pele_clara::coração_ve...\n",
       "10                                                MENÇÃO\n",
       "12     MENÇÃO simplesmente : coração_vermelho::coraçã...\n",
       "...                                                  ...\n",
       "22498  ovotoésecreto \\n  deputada estadual MENÇÃO \\n ...\n",
       "22500  MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO casa vc MEN...\n",
       "22501        MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO sairão ♡\n",
       "22502  candidatos pra estadual federal respectivament...\n",
       "22504  MENÇÃO MENÇÃO ownnn conte comigo : coração_ver...\n",
       "\n",
       "[19390 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para lemmatização\n",
    "def lemmatizar(texto):\n",
    "    doc = nlp(texto)\n",
    "    lemmatized_tokens = []\n",
    "    for token in doc:\n",
    "        lemmatized_tokens.append(token.lemma_)\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tweets_lemma'] = df_tweets['rawContent'].apply(lemmatizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>tweets_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MENÇÃO simplesmente : coração_vermelho::coraçã...</td>\n",
       "      <td>MENÇÃO simplesmente : coração_vermelho::coraçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MENÇÃO</td>\n",
       "      <td>MENÇÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MENÇÃO : mão_beliscando_pele_clara::coração_ve...</td>\n",
       "      <td>MENÇÃO : mão_beliscando_pele_clara::coração_ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MENÇÃO</td>\n",
       "      <td>MENÇÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MENÇÃO simplesmente : coração_vermelho::coraçã...</td>\n",
       "      <td>MENÇÃO simplesmente : coração_vermelho::coraçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>ovotoésecreto \\n  deputada estadual MENÇÃO \\n ...</td>\n",
       "      <td>ovotoésecreto \\n   deputada estadual MENÇÃO \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO casa vc MEN...</td>\n",
       "      <td>MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO casar vc ME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO sairão ♡</td>\n",
       "      <td>MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO sair ♡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivament...</td>\n",
       "      <td>candidato pra estadual federal respectivamente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>MENÇÃO MENÇÃO ownnn conte comigo : coração_ver...</td>\n",
       "      <td>MENÇÃO MENÇÃO ownnn contar comigo : coração_ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19390 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent  \\\n",
       "1      MENÇÃO simplesmente : coração_vermelho::coraçã...   \n",
       "4                                                 MENÇÃO   \n",
       "6      MENÇÃO : mão_beliscando_pele_clara::coração_ve...   \n",
       "10                                                MENÇÃO   \n",
       "12     MENÇÃO simplesmente : coração_vermelho::coraçã...   \n",
       "...                                                  ...   \n",
       "22498  ovotoésecreto \\n  deputada estadual MENÇÃO \\n ...   \n",
       "22500  MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO casa vc MEN...   \n",
       "22501        MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO sairão ♡   \n",
       "22502  candidatos pra estadual federal respectivament...   \n",
       "22504  MENÇÃO MENÇÃO ownnn conte comigo : coração_ver...   \n",
       "\n",
       "                                            tweets_lemma  \n",
       "1      MENÇÃO simplesmente : coração_vermelho::coraçã...  \n",
       "4                                                 MENÇÃO  \n",
       "6      MENÇÃO : mão_beliscando_pele_clara::coração_ve...  \n",
       "10                                                MENÇÃO  \n",
       "12     MENÇÃO simplesmente : coração_vermelho::coraçã...  \n",
       "...                                                  ...  \n",
       "22498  ovotoésecreto \\n   deputada estadual MENÇÃO \\n...  \n",
       "22500  MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO casar vc ME...  \n",
       "22501          MENÇÃO MENÇÃO MENÇÃO MENÇÃO MENÇÃO sair ♡  \n",
       "22502  candidato pra estadual federal respectivamente...  \n",
       "22504  MENÇÃO MENÇÃO ownnn contar comigo : coração_ve...  \n",
       "\n",
       "[19390 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para classificação de palavras\n",
    "def pos_tag(texto):\n",
    "    doc = nlp(texto)\n",
    "    pos_tags = []\n",
    "    for token in doc:\n",
    "        pos_tags.append((token.text, token.pos_))\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tweets_pos'] = df_tweets['rawContent'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(longo, ADJ), (dia, NOUN), (recebendo, VERB),...\n",
       "1        [(  , SPACE), (simplesmente, ADV), (❤, NOUN), ...\n",
       "2        [(  , SPACE), (hahahhaha, NOUN), (💜, PUNCT), (...\n",
       "3        [(longo, ADJ), (dia, NOUN), (recebendo, VERB),...\n",
       "5                              [(  , SPACE), (🫶🏾🫶🏾, NOUN)]\n",
       "                               ...                        \n",
       "22500    [(      , SPACE), (casa, NOUN), (vc, PROPN), (...\n",
       "22501         [(      , SPACE), (sairão, VERB), (♡, NOUN)]\n",
       "22502    [(candidatos, NOUN), (pra, ADP), (estadual, NO...\n",
       "22503    [(   , SPACE), (força, NOUN), (fabio, PROPN), ...\n",
       "22504    [(   , SPACE), (ownnn, NOUN), (conte, VERB), (...\n",
       "Name: tweets_pos, Length: 21357, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweets_pos']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Completo\n",
    "Para facilitar e aplicar todas as transformações e limpezas foi proposto pipeline, porém ele ainda não funciona.\n",
    "Caso queiram aplicar mandem bala. Em uma das conversas com ChatGPT uma hipotese levantada foi de que o pipeline não suporta emojis e caracteres especial. Então precisariamos de uma limpeza maior. Ai entra outro ponto, se tirarmos todos esses caracteres podemos processar com mais eficacia, porém perderiamos informações no meio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# def pos_tagging(text):\n",
    "#     doc = nlp(text)\n",
    "#     pos_tags = []\n",
    "#     for token in doc:\n",
    "#         pos_tags.append(token.pos_)\n",
    "#     return pos_tags\n",
    "\n",
    "# def lemmatize_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     lemmatized_tokens = []\n",
    "#     for token in doc:\n",
    "#         lemmatized_tokens.append(token.lemma_)\n",
    "#     return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# def create_tfidf_matrix(texts):\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "#     return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('pos_tagging', FunctionTransformer(pos_tagging)),\n",
    "#     ('lemmatization', FunctionTransformer(lemmatize_text)),\n",
    "#     ('tfidf', FunctionTransformer(create_tfidf_matrix)),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_data = pipeline.fit_transform(df_tweets_filt['rawContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweets['pos_tagged'] = df_tweets['rawContent'].apply(pos_tagging)\n",
    "# df_tweets['lemma'] = df_tweets['pos_tagged'].apply(lemmatize_text)\n",
    "# df_tweets['tfidf_matrix'] = df_tweets['lemma'].apply(create_tfidf_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos não supervisionados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-based Approach (CBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar e preprocessar\n",
    "tfidf = TfidfVectorizer(strip_accents='ascii', lowercase=True,\n",
    "                        ngram_range=(1,5))\n",
    "X = tfidf.fit_transform(df_tweets['tweets_lemma'])\n",
    "\n",
    "# Treinar cluster para agrupar tweets similares\n",
    "k = 2  # numero de clusters\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Rotular (não entendi exatamente porque e como estipularam 0 para positivo)\n",
    "df_tweets['classif_cba'] = ['positive' if label == 0 else 'negative' for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    17798\n",
       "negative     1592\n",
       "Name: classif_cba, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_cba'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling-based Approach (TMBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar e preprocessar\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df_tweets['tweets_lemma'])\n",
    "\n",
    "# Treinar LDA para identificar topicos/temas\n",
    "n_topics = 2  # numero de topicos\n",
    "lda = LatentDirichletAllocation(n_components=n_topics)\n",
    "lda.fit(X)\n",
    "\n",
    "# Manualmente atribuir rotulos para cada topico\n",
    "topic_sentiments = ['positive', 'negative']\n",
    "topic_labels = [topic_sentiments[i] for i in lda.transform(X).argmax(axis=1)]\n",
    "\n",
    "# Rotular tweets baseado em topicos\n",
    "df_tweets['classif_tmba'] = topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    12039\n",
       "negative     9318\n",
       "Name: classif_tmba, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_tmba'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Intensity Analysis (SIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\55199\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon') # download do sentiment lexicon\n",
    "\n",
    "sia = SentimentIntensityAnalyzer() # criar sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop pelos tweets e retornar score de sentimento\n",
    "sentiments = []\n",
    "\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    sentiment = sia.polarity_scores(tweet)\n",
    "    sentiments.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['sentiment_sia'] = sentiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos supervisionados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para classificação usando sentiment140\n",
    "def classif_sent140(tweet):\n",
    "    analise = TextBlob(tweet)\n",
    "    if analise.sentiment.polarity > 0:\n",
    "        return 'positivo'\n",
    "    elif analise.sentiment.polarity == 0:\n",
    "        return 'neutro'\n",
    "    else:\n",
    "        return 'negativo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['classif_sent140'] = df_tweets['tweets_lemma'].apply(classif_sent140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutro      20191\n",
       "positivo      927\n",
       "negativo      239\n",
       "Name: classif_sent140, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_sent140'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTimbau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 43.0/43.0 [00:00<00:00, 4.68kB/s]\n",
      "d:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\55199\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 647/647 [00:00<00:00, 107kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 210k/210k [00:00<00:00, 339kB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 515B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 12.8kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:11<00:00, 36.7MB/s] \n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Carregar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Carregar modelo pré treinado BERTimbau\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atribuir análise ao CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Função para analise sentimento\n",
    "def predict_sentiment(tweet):\n",
    "    # Encode do tweet com tokenizer do BERT\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    # Input IDs and attention mask (não entendi exatamente essa parte)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    # Fazer previsão com BERTimbau\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    # Rotulos de classificação (positivo ou negativo)\n",
    "    predicted_label = torch.argmax(outputs[0]).item()\n",
    "    # Returnar rotulos previstos\n",
    "    return \"positivo\" if predicted_label == 1 else \"negativo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista vazia para armazenar classificação de cada tweet\n",
    "predicted_sentiments = []\n",
    "\n",
    "# Loop por cada tweet e previsão\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    predicted_sentiment = predict_sentiment(tweet)\n",
    "    predicted_sentiments.append(predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[\"classif_bertimbau\"] = predicted_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    20571\n",
       "positive      786\n",
       "Name: predicted_sentiment, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets[\"classif_bertimbau\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.rename(columns={'predicted_sentiment':'classif_bertimbau'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa linha abaixo eu quis criar uma coluna com o compound da classificação SIA, pois o resultado do modelo é um dicionário com os valores que o modelo calculou para cada sentimento(neutro, positivo e negativo, sendo o compound uma metrica relativa aos 3 sentimentos presentes no texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4004766246.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[98], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Nessa linha eu quis criar uma coluna com o compound da classificação SIA, pois o resultado do modelo é um dicionário com os valores que o modelo calculou para cada sentimento(neutro, positivo e negativo, sendo o compound uma metrica relativa aos 3 sentimentos presentes no texto)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "\n",
    "for result in df_tweets['sentiment_sia']:\n",
    "    compound = result['compound']\n",
    "    list.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['compound_sia'] = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.rename(columns={'sentiment_sia':'classif_sia'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slavando resultado\n",
    "df_tweets.to_csv('data/df_tweets_classif.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando dataframe\n",
    "df_novo = pd.read_csv('data/df_tweets_classif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>tweets_lemma</th>\n",
       "      <th>tweets_pos</th>\n",
       "      <th>classif_cba</th>\n",
       "      <th>classif_tmba</th>\n",
       "      <th>classif_sia</th>\n",
       "      <th>classif_sent140</th>\n",
       "      <th>classif_bertimbau</th>\n",
       "      <th>compound_sia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longo dia recebendo denúncias pessoas presas h...</td>\n",
       "      <td>longo dia receber denúncia pessoa presas haver...</td>\n",
       "      <td>[('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simplesmente ❤ ️ ❤ ️ ❤ ️</td>\n",
       "      <td>simplesmente ❤ ️ ❤ ️ ❤ ️</td>\n",
       "      <td>[('  ', 'SPACE'), ('simplesmente', 'ADV'), ('❤...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hahahhaha 💜 💜 💜</td>\n",
       "      <td>hahahhaha 💜 💜 💜</td>\n",
       "      <td>[('  ', 'SPACE'), ('hahahhaha', 'NOUN'), ('💜',...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>longo dia recebendo denúncias pessoas presas h...</td>\n",
       "      <td>longo dia receber denúncia pessoa presas haver...</td>\n",
       "      <td>[('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>🫶🏾🫶🏾</td>\n",
       "      <td>🫶🏾🫶🏾</td>\n",
       "      <td>[('  ', 'SPACE'), ('\\U0001faf6🏾\\U0001faf6🏾', '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>casa vc   opções mãe tias candidatas saí...</td>\n",
       "      <td>casa vc    opção mãe tia candidato sair...</td>\n",
       "      <td>[('      ', 'SPACE'), ('casa', 'NOUN'), ('vc',...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compou...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>sairão ♡</td>\n",
       "      <td>sair ♡</td>\n",
       "      <td>[('      ', 'SPACE'), ('sairão', 'VERB'), ('♡'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivamente</td>\n",
       "      <td>candidato pra estadual federal respectivamente</td>\n",
       "      <td>[('candidatos', 'NOUN'), ('pra', 'ADP'), ('est...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>força fabio 💜</td>\n",
       "      <td>força Fabio 💜</td>\n",
       "      <td>[('   ', 'SPACE'), ('força', 'NOUN'), ('fabio'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>ownnn conte comigo ❤</td>\n",
       "      <td>ownnn contar comigo ❤</td>\n",
       "      <td>[('   ', 'SPACE'), ('ownnn', 'NOUN'), ('conte'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21357 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent  \\\n",
       "0      longo dia recebendo denúncias pessoas presas h...   \n",
       "1                               simplesmente ❤ ️ ❤ ️ ❤ ️   \n",
       "2                                        hahahhaha 💜 💜 💜   \n",
       "3      longo dia recebendo denúncias pessoas presas h...   \n",
       "5                                                   🫶🏾🫶🏾   \n",
       "...                                                  ...   \n",
       "22500        casa vc   opções mãe tias candidatas saí...   \n",
       "22501                                           sairão ♡   \n",
       "22502  candidatos pra estadual federal respectivamente     \n",
       "22503                                      força fabio 💜   \n",
       "22504                               ownnn conte comigo ❤   \n",
       "\n",
       "                                            tweets_lemma  \\\n",
       "0      longo dia receber denúncia pessoa presas haver...   \n",
       "1                               simplesmente ❤ ️ ❤ ️ ❤ ️   \n",
       "2                                        hahahhaha 💜 💜 💜   \n",
       "3      longo dia receber denúncia pessoa presas haver...   \n",
       "5                                                   🫶🏾🫶🏾   \n",
       "...                                                  ...   \n",
       "22500         casa vc    opção mãe tia candidato sair...   \n",
       "22501                                             sair ♡   \n",
       "22502   candidato pra estadual federal respectivamente     \n",
       "22503                                      força Fabio 💜   \n",
       "22504                              ownnn contar comigo ❤   \n",
       "\n",
       "                                              tweets_pos classif_cba  \\\n",
       "0      [('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...    positive   \n",
       "1      [('  ', 'SPACE'), ('simplesmente', 'ADV'), ('❤...    positive   \n",
       "2      [('  ', 'SPACE'), ('hahahhaha', 'NOUN'), ('💜',...    positive   \n",
       "3      [('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...    positive   \n",
       "5      [('  ', 'SPACE'), ('\\U0001faf6🏾\\U0001faf6🏾', '...    positive   \n",
       "...                                                  ...         ...   \n",
       "22500  [('      ', 'SPACE'), ('casa', 'NOUN'), ('vc',...    positive   \n",
       "22501  [('      ', 'SPACE'), ('sairão', 'VERB'), ('♡'...    positive   \n",
       "22502  [('candidatos', 'NOUN'), ('pra', 'ADP'), ('est...    positive   \n",
       "22503  [('   ', 'SPACE'), ('força', 'NOUN'), ('fabio'...    positive   \n",
       "22504  [('   ', 'SPACE'), ('ownnn', 'NOUN'), ('conte'...    positive   \n",
       "\n",
       "      classif_tmba                                        classif_sia  \\\n",
       "0         negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "1         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "2         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "3         negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "5         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "...            ...                                                ...   \n",
       "22500     positive  {'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compou...   \n",
       "22501     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22502     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22503     negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22504     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "\n",
       "      classif_sent140 classif_bertimbau  compound_sia  \n",
       "0              neutro          negative        0.0000  \n",
       "1              neutro          negative        0.0000  \n",
       "2              neutro          negative        0.0000  \n",
       "3              neutro          negative        0.0000  \n",
       "5              neutro          negative        0.0000  \n",
       "...               ...               ...           ...  \n",
       "22500          neutro          negative        0.5106  \n",
       "22501          neutro          negative        0.0000  \n",
       "22502          neutro          negative        0.0000  \n",
       "22503          neutro          negative        0.0000  \n",
       "22504          neutro          negative        0.0000  \n",
       "\n",
       "[21357 rows x 9 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2852de53f51e57db0fb3c92b7d03cd28fd1e6acc643c7bf23f4addc7db48be7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
