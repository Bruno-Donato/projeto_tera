{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas e dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import pt_core_news_lg\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55199\\AppData\\Local\\Temp\\ipykernel_18580\\1077869811.py:1: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/erika_hilton.csv', on_bad_lines='skip', sep=';', encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/erika_hilton.csv', on_bad_lines='skip', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22505, 49)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrar tweets?\n",
    "Pensei em filtrar os tweets, deixando somente os tweets direcionados a Erika, e para remover duplicatas.\n",
    "Nessa fase n√£o apliquei somente pra termos mais tweets para an√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usuarios = df[df['user'] != \"https://twitter.com/ErikakHilton\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19390, 49)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_usuarios.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo \"n√£o\" das stopwords\n",
    "STOP_WORDS.remove('n√£o')\n",
    "\n",
    "# Instanciando spacy/pt_core_news_lg - corpus completo\n",
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doze', 'fazeis', 'e', 'depois', 'antes', 'com', 'pode', '√∫ltimo', 'tr√™s', 'l√°', 'menor', 'tipo', 'porquanto', 'todo', 'final', 'parece', 'ora', 'vem', 'faz', 'segundo', 'ser', 'outras', 'isso', 'fora', 'disso', 'estivestes', 'p√¥de', 'terceiro', 'vezes', 'aquelas', 'sexta', 'em', 'portanto', 'tens', 'forma', 'geral', 'contudo', 'vos', 'pouca', 'sabe', 'treze', 'apoia', '√†', 'quarta', 'dentro', 'poder√°', 'comprida', 'usa', 'vais', 'no', 'seis', 'fazes', 'a√≠', 'local', 'v√£o', 'cujo', 'primeiro', 'comprido', 'devem', 'estes', 'muito', 'boa', 'nove', 'quem', 'outros', 'muitos', 'v√≥s', 'sem', 'quinta', 'pegar', 'apenas', 'zero', 'pelo', 'al√©m', 'agora', 'talvez', 'acerca', 'daquela', 'teu', 'caminho', 'porqu√™', 'cada', 'quatro', 'seus', 'tanta', 'quanto', 'ap√≥s', 'ir', 'adeus', 'grande', 'mais', 'dez', 'te', 'esses', 'num', 'novos', 'tudo', 'suas', 'perto', 'dezasseis', 'numa', 'onze', 'elas', 'quero', 'tivemos', 'veja', 'estiveram', 'ou', 'nosso', 'partir', 'logo', 'os', 'possivelmente', 'esteve', 'j√°', 'vens', 'quer', 'diz', 'dizem', 'vinda', 'neste', 'nesse', 's√≥', 'foste', 'segunda', 'querem', 'coisa', 'dezanove', 'enquanto', 'at√©', 'tivestes', 'todas', 'oitavo', 'custa', 'vindo', 'estado', 'estou', 'conhecida', 'meus', 'nas', 'entre', 'sei', 'inicio', 'dezoito', 'de', 'sou', 'nesta', 'seu', 'foi', 'pr√≥ximo', 'sexto', 'grupo', 'isto', 'este', 'tua', 'sete', 's√£o', '√†s', 'n√∫mero', 'nunca', 'fomos', 'assim', 'menos', 'duas', 'p√µem', 'fazem', 'cuja', 'm√™s', 'vai', 'fazemos', 'grandes', '√©s', 'mal', 'tanto', 'pr√≥xima', 'tentar', 'sobre', 'tais', 'vossos', 'novas', 'nessa', 'minha', 'alguns', 'pelos', 'ademais', 'quest√£o', 'sempre', 'quinze', 'nuns', 'breve', 'eu', 'tal', 'quinto', 'cima', 'do', 'meses', 'certamente', 'as', 'dos', 'voc√™', 'na', 'bastante', 'naquele', 'atr√°s', 'ambos', 'dezassete', 'posi√ß√£o', 'mas', 'far√°', 'me', 'nada', 'aos', 'tentei', 'n√∫meros', 'pontos', 'uma', 'lhe', 'for', 'vossa', 'atrav√©s', 'desde', 'vez', 'ela', 'cento', 'cedo', '√°rea', 'nenhuma', 'esse', 'povo', 'daquele', 'est√£o', 'mesmo', 'fostes', 'podia', 'das', 'uns', 'voc√™s', 'nova', 'temos', 'rela√ß√£o', 'desse', 'para', 'longe', 'cinco', 'ambas', 'desta', 'se', 'quarto', 'tu', 'pois', 'tempo', 'debaixo', 'estiveste', 'momento', 'pelas', 'bem', 'contra', 'inclusive', 'sua', 'ent√£o', 'sim', 'diante', 'poss√≠vel', 'qual', 'quais', 'quieta', 'p√µe', 'baixo', 'seria', 'est√°', 'embora', 'tendes', 'tiveste', 'terceira', 'lugar', 'aquela', 'como', 'fazer', 'deve', 'aqui', 'da', 'nem', 'fa√ßo', 'toda', 'usar', 'umas', 'poder', 'ainda', 'ponto', 'aquele', 'tamb√©m', 'por√©m', 'm√°ximo', 'posso', 'aquilo', 'sistema', 'tem', 'n√≠vel', 'parte', 'tentaram', 'somos', 'ele', 'n√≥s', 'estar√°', 'foram', 'dois', 'teve', 'ali', 'naquela', 'estar', 'oito', 'eles', 'quieto', 'a', 'dizer', 'ver', 'algo', 'onde', 't√£o', 'estas', 'conhecido', 'certeza', 'valor', 'pela', 'maioria', 'apoio', 'pr√≥prio', 't√™m', 'maiorias', 'bom', 'teus', 'conselho', 'quando', 'v√™m', 's√©tima', 'falta', 'tenho', 'o', 'essas', 'tarde', 'direita', 'fazia', '√©', 'qualquer', 'novo', 'nos', 'est√°s', 'iniciar', 'ter', 'maior', 'favor', 'essa', 'fez', 'outra', 'minhas', 'vinte', 'meu', 'eventual', 'vosso', 'corrente', 'meio', 'estive', 'des', 'por', 'somente', 'oitava', 'que', 'saber', 'nossas', 'esta', 'qu√™', 'tive', 'dever√°', 'tente', 'pouco', 'nossos', 'tiveram', 'vossas', 'dar', 'tuas', 'exemplo', 'demais', 'ir√°', 'obrigada', 'porque', 'deste', 'algumas', 'd√£o', 'estivemos', 'v√°rios', 'estava', 'mil', 'ligado', 'd√°', 'primeira', 'ao', 'nossa', 'ontem', 'puderam', 'sob', 'todos', 'c√°', 'um', 'era', 'lado', 'aqueles', 's√©timo', 'sois', 'dessa', 'catorze', 'apontar', 'fui', 'obrigado', 'fim', 'podem'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_usuarios['rawContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11744                       @ErikakHilton @silviokiwi DIVA\n",
       "9395     @ErikakHilton √â mais f√°cil o eleitor mudar de ...\n",
       "12798                @ErikakHilton https://t.co/Qd5cKP4qm2\n",
       "18809    @ErikakHilton @cortezpsol @GuilhermeBoulos @bf...\n",
       "20870    @ErikakHilton T√¥ indo mami. Agora √© a hora \\n1...\n",
       "6699     @ErikakHilton √â bom lembrar que se gastar mais...\n",
       "17650    @ErikakHilton @_danielaabade Ler os coment√°rio...\n",
       "13622              @ErikakHilton √© de muita HIPOCRISIA !!!\n",
       "3602                 @ErikakHilton #ForaBolsonaroMentiroso\n",
       "6051     @ErikakHilton PQP!!!!\\nPQP!!!!\\nPQP!!!!\\nPQP!!...\n",
       "Name: rawContent, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remover URL\n",
    "def remover_url(texto):\n",
    "    texto = re.sub(r'http\\S+', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remover contas\n",
    "def remover_twiters(texto):\n",
    "    texto = re.sub(r'@\\w+', '@MEN√á√ÉO', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remover pontua√ß√µes\n",
    "def remover_pontuacao(texto):\n",
    "    texto = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demojizar(x):\n",
    "    x = emoji.demojize(x, language='pt')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando limpezas\n",
    "tweets = tweets.str.lower()\n",
    "tweets = tweets.apply(remover_url)\n",
    "tweets = tweets.apply(remover_twiters)\n",
    "tweets = tweets.apply(remover_pontuacao)\n",
    "tweets = tweets.apply(demojizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remo√ß√£o de stopwords e digitos\n",
    "def limpar_texto(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_digit]\n",
    "    texto_limpo = \" \".join(tokens)\n",
    "    return texto_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.apply(limpar_texto)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentativa de corrigir typos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essa biblioteca seria possivel corrigir erros de digita√ß√£o.<br>\n",
    "Em alguns tweets percebi que tinham palavras com letras repetidas ou contra√ß√µes e isso poderia dificutar a nossa an√°lise.<br>\n",
    "Mas n√£o consegui aplicar essa etapa da limpeza. Vou deixar marcado caso alguem queira tentar aplicar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando spellchecker\n",
    "spell = SpellChecker(language='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para corre√ß√£o de grafia\n",
    "def corretor_grafia(texto):\n",
    "    tb = TextBlob(texto)\n",
    "    corrected_words = []\n",
    "    for word in tb.words:\n",
    "        corrected_word = word.correct()\n",
    "        corrected_words.append(corrected_word)\n",
    "    \n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        longo dia recebendo den√∫ncias pessoas presas h...\n",
       "1          simplesmente : cora√ß√£o_vermelho::cora√ß√£o_ver...\n",
       "2          hahahhaha : cora√ß√£o_roxo::cora√ß√£o_roxo::cora...\n",
       "3        longo dia recebendo den√∫ncias pessoas presas h...\n",
       "4                                                         \n",
       "                               ...                        \n",
       "22500          casa vc   op√ß√µes m√£e tias candidatas sa√≠...\n",
       "22501                                             sair√£o ‚ô°\n",
       "22502    candidatos pra estadual federal respectivamente  \n",
       "22503                         for√ßa fabio : cora√ß√£o_roxo :\n",
       "22504              ownnn conte comigo : cora√ß√£o_vermelho :\n",
       "Name: rawContent, Length: 22505, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets \u001b[39m=\u001b[39m tweets\u001b[39m.\u001b[39;49mapply(corretor_grafia)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mcorretor_grafia\u001b[1;34m(texto)\u001b[0m\n\u001b[0;32m      4\u001b[0m corrected_words \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tb\u001b[39m.\u001b[39mwords:\n\u001b[1;32m----> 6\u001b[0m     corrected_word \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39;49mcorrect()\n\u001b[0;32m      7\u001b[0m     corrected_words\u001b[39m.\u001b[39mappend(corrected_word)\n\u001b[0;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m corrected_words\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\blob.py:142\u001b[0m, in \u001b[0;36mWord.correct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Correct the spelling of the word. Returns the word with the highest\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    confidence using the spelling corrector.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m Word(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspellcheck()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\blob.py:134\u001b[0m, in \u001b[0;36mWord.spellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspellcheck\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    126\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Return a list of (word, confidence) tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[39m    Based on: Peter Norvig, \"How to Write a Spelling Corrector\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m suggest(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\en\\__init__.py:123\u001b[0m, in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msuggest\u001b[39m(w):\n\u001b[0;32m    121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m spelling\u001b[39m.\u001b[39;49msuggest(w)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:1399\u001b[0m, in \u001b[0;36mSpelling.suggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m w\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misdigit():\n\u001b[0;32m   1396\u001b[0m     \u001b[39mreturn\u001b[39;00m [(w, \u001b[39m1.0\u001b[39m)] \u001b[39m# 1.5\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known([w]) \\\n\u001b[0;32m   1398\u001b[0m           \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w)) \\\n\u001b[1;32m-> 1399\u001b[0m           \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit2(w)) \\\n\u001b[0;32m   1400\u001b[0m           \u001b[39mor\u001b[39;00m [w]\n\u001b[0;32m   1401\u001b[0m candidates \u001b[39m=\u001b[39m [(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(c, \u001b[39m0.0\u001b[39m), c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m candidates]\n\u001b[0;32m   1402\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39msum\u001b[39m(p \u001b[39mfor\u001b[39;00m p, word \u001b[39min\u001b[39;00m candidates) \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:1376\u001b[0m, in \u001b[0;36mSpelling._edit2\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39;49m(e2 \u001b[39mfor\u001b[39;49;00m e1 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit1(w) \u001b[39mfor\u001b[39;49;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edit1(e1) \u001b[39mif\u001b[39;49;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:1376\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Returns a set of words with edit distance 2 from the given word\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m \u001b[39m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Only keep candidates that are actually known words (20% speedup).\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mset\u001b[39m(e2 \u001b[39mfor\u001b[39;00m e1 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(w) \u001b[39mfor\u001b[39;00m e2 \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_edit1(e1) \u001b[39mif\u001b[39;00m e2 \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:96\u001b[0m, in \u001b[0;36mlazydict.__contains__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__contains__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy(\u001b[39m\"\u001b[39;49m\u001b[39m__contains__\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32md:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\textblob\\_text.py:84\u001b[0m, in \u001b[0;36mlazydict._lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lazy\u001b[39m(\u001b[39mself\u001b[39m, method, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     81\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" If the dictionary is empty, calls lazydict.load().\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m        Replaces lazydict.method() with dict.method() and calls it.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mdict\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__len__\u001b[39;49m(\u001b[39mself\u001b[39;49m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     85\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload()\n\u001b[0;32m     86\u001b[0m         \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, method, types\u001b[39m.\u001b[39mMethodType(\u001b[39mgetattr\u001b[39m(\u001b[39mdict\u001b[39m, method), \u001b[39mself\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweets = tweets.apply(corretor_grafia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando em dataframe\n",
    "df_tweets = tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEN√á√ÉO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MEN√á√ÉO : m√£o_beliscando_pele_clara::cora√ß√£o_ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MEN√á√ÉO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>ovoto√©secreto \\n  deputada estadual MEN√á√ÉO \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO casa vc MEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO sair√£o ‚ô°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO ownnn conte comigo : cora√ß√£o_ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19390 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent\n",
       "1      MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...\n",
       "4                                                 MEN√á√ÉO\n",
       "6      MEN√á√ÉO : m√£o_beliscando_pele_clara::cora√ß√£o_ve...\n",
       "10                                                MEN√á√ÉO\n",
       "12     MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...\n",
       "...                                                  ...\n",
       "22498  ovoto√©secreto \\n  deputada estadual MEN√á√ÉO \\n ...\n",
       "22500  MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO casa vc MEN...\n",
       "22501        MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO sair√£o ‚ô°\n",
       "22502  candidatos pra estadual federal respectivament...\n",
       "22504  MEN√á√ÉO MEN√á√ÉO ownnn conte comigo : cora√ß√£o_ver...\n",
       "\n",
       "[19390 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando e excluindo linhas vazias. \n",
    "# Aqui n√£o foi poss√≠vel excluir nem identificar com fun√ß√µes NA, pq a linhas estava vazia.\n",
    "df_tweets = df_tweets[df_tweets['rawContent'] != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEN√á√ÉO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MEN√á√ÉO : m√£o_beliscando_pele_clara::cora√ß√£o_ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MEN√á√ÉO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>ovoto√©secreto \\n  deputada estadual MEN√á√ÉO \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO casa vc MEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO sair√£o ‚ô°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO ownnn conte comigo : cora√ß√£o_ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19390 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent\n",
       "1      MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...\n",
       "4                                                 MEN√á√ÉO\n",
       "6      MEN√á√ÉO : m√£o_beliscando_pele_clara::cora√ß√£o_ve...\n",
       "10                                                MEN√á√ÉO\n",
       "12     MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...\n",
       "...                                                  ...\n",
       "22498  ovoto√©secreto \\n  deputada estadual MEN√á√ÉO \\n ...\n",
       "22500  MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO casa vc MEN...\n",
       "22501        MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO sair√£o ‚ô°\n",
       "22502  candidatos pra estadual federal respectivament...\n",
       "22504  MEN√á√ÉO MEN√á√ÉO ownnn conte comigo : cora√ß√£o_ver...\n",
       "\n",
       "[19390 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para lemmatiza√ß√£o\n",
    "def lemmatizar(texto):\n",
    "    doc = nlp(texto)\n",
    "    lemmatized_tokens = []\n",
    "    for token in doc:\n",
    "        lemmatized_tokens.append(token.lemma_)\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tweets_lemma'] = df_tweets['rawContent'].apply(lemmatizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>tweets_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...</td>\n",
       "      <td>MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEN√á√ÉO</td>\n",
       "      <td>MEN√á√ÉO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MEN√á√ÉO : m√£o_beliscando_pele_clara::cora√ß√£o_ve...</td>\n",
       "      <td>MEN√á√ÉO : m√£o_beliscando_pele_clara::cora√ß√£o_ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MEN√á√ÉO</td>\n",
       "      <td>MEN√á√ÉO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...</td>\n",
       "      <td>MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>ovoto√©secreto \\n  deputada estadual MEN√á√ÉO \\n ...</td>\n",
       "      <td>ovoto√©secreto \\n   deputada estadual MEN√á√ÉO \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO casa vc MEN...</td>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO casar vc ME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO sair√£o ‚ô°</td>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO sair ‚ô°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivament...</td>\n",
       "      <td>candidato pra estadual federal respectivamente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO ownnn conte comigo : cora√ß√£o_ver...</td>\n",
       "      <td>MEN√á√ÉO MEN√á√ÉO ownnn contar comigo : cora√ß√£o_ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19390 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent  \\\n",
       "1      MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...   \n",
       "4                                                 MEN√á√ÉO   \n",
       "6      MEN√á√ÉO : m√£o_beliscando_pele_clara::cora√ß√£o_ve...   \n",
       "10                                                MEN√á√ÉO   \n",
       "12     MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...   \n",
       "...                                                  ...   \n",
       "22498  ovoto√©secreto \\n  deputada estadual MEN√á√ÉO \\n ...   \n",
       "22500  MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO casa vc MEN...   \n",
       "22501        MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO sair√£o ‚ô°   \n",
       "22502  candidatos pra estadual federal respectivament...   \n",
       "22504  MEN√á√ÉO MEN√á√ÉO ownnn conte comigo : cora√ß√£o_ver...   \n",
       "\n",
       "                                            tweets_lemma  \n",
       "1      MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...  \n",
       "4                                                 MEN√á√ÉO  \n",
       "6      MEN√á√ÉO : m√£o_beliscando_pele_clara::cora√ß√£o_ve...  \n",
       "10                                                MEN√á√ÉO  \n",
       "12     MEN√á√ÉO simplesmente : cora√ß√£o_vermelho::cora√ß√£...  \n",
       "...                                                  ...  \n",
       "22498  ovoto√©secreto \\n   deputada estadual MEN√á√ÉO \\n...  \n",
       "22500  MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO casar vc ME...  \n",
       "22501          MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO MEN√á√ÉO sair ‚ô°  \n",
       "22502  candidato pra estadual federal respectivamente...  \n",
       "22504  MEN√á√ÉO MEN√á√ÉO ownnn contar comigo : cora√ß√£o_ve...  \n",
       "\n",
       "[19390 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para classifica√ß√£o de palavras\n",
    "def pos_tag(texto):\n",
    "    doc = nlp(texto)\n",
    "    pos_tags = []\n",
    "    for token in doc:\n",
    "        pos_tags.append((token.text, token.pos_))\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tweets_pos'] = df_tweets['rawContent'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(longo, ADJ), (dia, NOUN), (recebendo, VERB),...\n",
       "1        [(  , SPACE), (simplesmente, ADV), (‚ù§, NOUN), ...\n",
       "2        [(  , SPACE), (hahahhaha, NOUN), (üíú, PUNCT), (...\n",
       "3        [(longo, ADJ), (dia, NOUN), (recebendo, VERB),...\n",
       "5                              [(  , SPACE), (ü´∂üèæü´∂üèæ, NOUN)]\n",
       "                               ...                        \n",
       "22500    [(      , SPACE), (casa, NOUN), (vc, PROPN), (...\n",
       "22501         [(      , SPACE), (sair√£o, VERB), (‚ô°, NOUN)]\n",
       "22502    [(candidatos, NOUN), (pra, ADP), (estadual, NO...\n",
       "22503    [(   , SPACE), (for√ßa, NOUN), (fabio, PROPN), ...\n",
       "22504    [(   , SPACE), (ownnn, NOUN), (conte, VERB), (...\n",
       "Name: tweets_pos, Length: 21357, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweets_pos']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Completo\n",
    "Para facilitar e aplicar todas as transforma√ß√µes e limpezas foi proposto pipeline, por√©m ele ainda n√£o funciona.\n",
    "Caso queiram aplicar mandem bala. Em uma das conversas com ChatGPT uma hipotese levantada foi de que o pipeline n√£o suporta emojis e caracteres especial. Ent√£o precisariamos de uma limpeza maior. Ai entra outro ponto, se tirarmos todos esses caracteres podemos processar com mais eficacia, por√©m perderiamos informa√ß√µes no meio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# def pos_tagging(text):\n",
    "#     doc = nlp(text)\n",
    "#     pos_tags = []\n",
    "#     for token in doc:\n",
    "#         pos_tags.append(token.pos_)\n",
    "#     return pos_tags\n",
    "\n",
    "# def lemmatize_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     lemmatized_tokens = []\n",
    "#     for token in doc:\n",
    "#         lemmatized_tokens.append(token.lemma_)\n",
    "#     return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# def create_tfidf_matrix(texts):\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "#     return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('pos_tagging', FunctionTransformer(pos_tagging)),\n",
    "#     ('lemmatization', FunctionTransformer(lemmatize_text)),\n",
    "#     ('tfidf', FunctionTransformer(create_tfidf_matrix)),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_data = pipeline.fit_transform(df_tweets_filt['rawContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweets['pos_tagged'] = df_tweets['rawContent'].apply(pos_tagging)\n",
    "# df_tweets['lemma'] = df_tweets['pos_tagged'].apply(lemmatize_text)\n",
    "# df_tweets['tfidf_matrix'] = df_tweets['lemma'].apply(create_tfidf_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos n√£o supervisionados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-based Approach (CBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar e preprocessar\n",
    "tfidf = TfidfVectorizer(strip_accents='ascii', lowercase=True,\n",
    "                        ngram_range=(1,5))\n",
    "X = tfidf.fit_transform(df_tweets['tweets_lemma'])\n",
    "\n",
    "# Treinar cluster para agrupar tweets similares\n",
    "k = 2  # numero de clusters\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Rotular (n√£o entendi exatamente porque e como estipularam 0 para positivo)\n",
    "df_tweets['classif_cba'] = ['positive' if label == 0 else 'negative' for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    17798\n",
       "negative     1592\n",
       "Name: classif_cba, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_cba'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling-based Approach (TMBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar e preprocessar\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df_tweets['tweets_lemma'])\n",
    "\n",
    "# Treinar LDA para identificar topicos/temas\n",
    "n_topics = 2  # numero de topicos\n",
    "lda = LatentDirichletAllocation(n_components=n_topics)\n",
    "lda.fit(X)\n",
    "\n",
    "# Manualmente atribuir rotulos para cada topico\n",
    "topic_sentiments = ['positive', 'negative']\n",
    "topic_labels = [topic_sentiments[i] for i in lda.transform(X).argmax(axis=1)]\n",
    "\n",
    "# Rotular tweets baseado em topicos\n",
    "df_tweets['classif_tmba'] = topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    12039\n",
       "negative     9318\n",
       "Name: classif_tmba, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_tmba'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Intensity Analysis (SIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\55199\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon') # download do sentiment lexicon\n",
    "\n",
    "sia = SentimentIntensityAnalyzer() # criar sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop pelos tweets e retornar score de sentimento\n",
    "sentiments = []\n",
    "\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    sentiment = sia.polarity_scores(tweet)\n",
    "    sentiments.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['sentiment_sia'] = sentiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos supervisionados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para classifica√ß√£o usando sentiment140\n",
    "def classif_sent140(tweet):\n",
    "    analise = TextBlob(tweet)\n",
    "    if analise.sentiment.polarity > 0:\n",
    "        return 'positivo'\n",
    "    elif analise.sentiment.polarity == 0:\n",
    "        return 'neutro'\n",
    "    else:\n",
    "        return 'negativo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['classif_sent140'] = df_tweets['tweets_lemma'].apply(classif_sent140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutro      20191\n",
       "positivo      927\n",
       "negativo      239\n",
       "Name: classif_sent140, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_sent140'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTimbau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43.0/43.0 [00:00<00:00, 4.68kB/s]\n",
      "d:\\ciencia_de_dados\\projetos\\tera_projeto\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\55199\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 647/647 [00:00<00:00, 107kB/s]\n",
      "Downloading (‚Ä¶)solve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210k/210k [00:00<00:00, 339kB/s]\n",
      "Downloading (‚Ä¶)in/added_tokens.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.00/2.00 [00:00<00:00, 515B/s]\n",
      "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 112/112 [00:00<00:00, 12.8kB/s]\n",
      "Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 438M/438M [00:11<00:00, 36.7MB/s] \n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Carregar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Carregar modelo pr√© treinado BERTimbau\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#atribuir an√°lise ao CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Fun√ß√£o para analise sentimento\n",
    "def predict_sentiment(tweet):\n",
    "    # Encode do tweet com tokenizer do BERT\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    # Input IDs and attention mask (n√£o entendi exatamente essa parte)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    # Fazer previs√£o com BERTimbau\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    # Rotulos de classifica√ß√£o (positivo ou negativo)\n",
    "    predicted_label = torch.argmax(outputs[0]).item()\n",
    "    # Returnar rotulos previstos\n",
    "    return \"positivo\" if predicted_label == 1 else \"negativo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista vazia para armazenar classifica√ß√£o de cada tweet\n",
    "predicted_sentiments = []\n",
    "\n",
    "# Loop por cada tweet e previs√£o\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    predicted_sentiment = predict_sentiment(tweet)\n",
    "    predicted_sentiments.append(predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[\"classif_bertimbau\"] = predicted_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    20571\n",
       "positive      786\n",
       "Name: predicted_sentiment, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets[\"classif_bertimbau\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.rename(columns={'predicted_sentiment':'classif_bertimbau'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa linha abaixo eu quis criar uma coluna com o compound da classifica√ß√£o SIA, pois o resultado do modelo √© um dicion√°rio com os valores que o modelo calculou para cada sentimento(neutro, positivo e negativo, sendo o compound uma metrica relativa aos 3 sentimentos presentes no texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4004766246.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[98], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Nessa linha eu quis criar uma coluna com o compound da classifica√ß√£o SIA, pois o resultado do modelo √© um dicion√°rio com os valores que o modelo calculou para cada sentimento(neutro, positivo e negativo, sendo o compound uma metrica relativa aos 3 sentimentos presentes no texto)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "\n",
    "for result in df_tweets['sentiment_sia']:\n",
    "    compound = result['compound']\n",
    "    list.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['compound_sia'] = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.rename(columns={'sentiment_sia':'classif_sia'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slavando resultado\n",
    "df_tweets.to_csv('data/df_tweets_classif.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando dataframe\n",
    "df_novo = pd.read_csv('data/df_tweets_classif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>tweets_lemma</th>\n",
       "      <th>tweets_pos</th>\n",
       "      <th>classif_cba</th>\n",
       "      <th>classif_tmba</th>\n",
       "      <th>classif_sia</th>\n",
       "      <th>classif_sent140</th>\n",
       "      <th>classif_bertimbau</th>\n",
       "      <th>compound_sia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longo dia recebendo den√∫ncias pessoas presas h...</td>\n",
       "      <td>longo dia receber den√∫ncia pessoa presas haver...</td>\n",
       "      <td>[('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è</td>\n",
       "      <td>simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è</td>\n",
       "      <td>[('  ', 'SPACE'), ('simplesmente', 'ADV'), ('‚ù§...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hahahhaha üíú üíú üíú</td>\n",
       "      <td>hahahhaha üíú üíú üíú</td>\n",
       "      <td>[('  ', 'SPACE'), ('hahahhaha', 'NOUN'), ('üíú',...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>longo dia recebendo den√∫ncias pessoas presas h...</td>\n",
       "      <td>longo dia receber den√∫ncia pessoa presas haver...</td>\n",
       "      <td>[('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ü´∂üèæü´∂üèæ</td>\n",
       "      <td>ü´∂üèæü´∂üèæ</td>\n",
       "      <td>[('  ', 'SPACE'), ('\\U0001faf6üèæ\\U0001faf6üèæ', '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>casa vc   op√ß√µes m√£e tias candidatas sa√≠...</td>\n",
       "      <td>casa vc    op√ß√£o m√£e tia candidato sair...</td>\n",
       "      <td>[('      ', 'SPACE'), ('casa', 'NOUN'), ('vc',...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compou...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>sair√£o ‚ô°</td>\n",
       "      <td>sair ‚ô°</td>\n",
       "      <td>[('      ', 'SPACE'), ('sair√£o', 'VERB'), ('‚ô°'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivamente</td>\n",
       "      <td>candidato pra estadual federal respectivamente</td>\n",
       "      <td>[('candidatos', 'NOUN'), ('pra', 'ADP'), ('est...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>for√ßa fabio üíú</td>\n",
       "      <td>for√ßa Fabio üíú</td>\n",
       "      <td>[('   ', 'SPACE'), ('for√ßa', 'NOUN'), ('fabio'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>ownnn conte comigo ‚ù§</td>\n",
       "      <td>ownnn contar comigo ‚ù§</td>\n",
       "      <td>[('   ', 'SPACE'), ('ownnn', 'NOUN'), ('conte'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21357 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent  \\\n",
       "0      longo dia recebendo den√∫ncias pessoas presas h...   \n",
       "1                               simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è   \n",
       "2                                        hahahhaha üíú üíú üíú   \n",
       "3      longo dia recebendo den√∫ncias pessoas presas h...   \n",
       "5                                                   ü´∂üèæü´∂üèæ   \n",
       "...                                                  ...   \n",
       "22500        casa vc   op√ß√µes m√£e tias candidatas sa√≠...   \n",
       "22501                                           sair√£o ‚ô°   \n",
       "22502  candidatos pra estadual federal respectivamente     \n",
       "22503                                      for√ßa fabio üíú   \n",
       "22504                               ownnn conte comigo ‚ù§   \n",
       "\n",
       "                                            tweets_lemma  \\\n",
       "0      longo dia receber den√∫ncia pessoa presas haver...   \n",
       "1                               simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è   \n",
       "2                                        hahahhaha üíú üíú üíú   \n",
       "3      longo dia receber den√∫ncia pessoa presas haver...   \n",
       "5                                                   ü´∂üèæü´∂üèæ   \n",
       "...                                                  ...   \n",
       "22500         casa vc    op√ß√£o m√£e tia candidato sair...   \n",
       "22501                                             sair ‚ô°   \n",
       "22502   candidato pra estadual federal respectivamente     \n",
       "22503                                      for√ßa Fabio üíú   \n",
       "22504                              ownnn contar comigo ‚ù§   \n",
       "\n",
       "                                              tweets_pos classif_cba  \\\n",
       "0      [('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...    positive   \n",
       "1      [('  ', 'SPACE'), ('simplesmente', 'ADV'), ('‚ù§...    positive   \n",
       "2      [('  ', 'SPACE'), ('hahahhaha', 'NOUN'), ('üíú',...    positive   \n",
       "3      [('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...    positive   \n",
       "5      [('  ', 'SPACE'), ('\\U0001faf6üèæ\\U0001faf6üèæ', '...    positive   \n",
       "...                                                  ...         ...   \n",
       "22500  [('      ', 'SPACE'), ('casa', 'NOUN'), ('vc',...    positive   \n",
       "22501  [('      ', 'SPACE'), ('sair√£o', 'VERB'), ('‚ô°'...    positive   \n",
       "22502  [('candidatos', 'NOUN'), ('pra', 'ADP'), ('est...    positive   \n",
       "22503  [('   ', 'SPACE'), ('for√ßa', 'NOUN'), ('fabio'...    positive   \n",
       "22504  [('   ', 'SPACE'), ('ownnn', 'NOUN'), ('conte'...    positive   \n",
       "\n",
       "      classif_tmba                                        classif_sia  \\\n",
       "0         negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "1         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "2         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "3         negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "5         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "...            ...                                                ...   \n",
       "22500     positive  {'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compou...   \n",
       "22501     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22502     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22503     negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22504     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "\n",
       "      classif_sent140 classif_bertimbau  compound_sia  \n",
       "0              neutro          negative        0.0000  \n",
       "1              neutro          negative        0.0000  \n",
       "2              neutro          negative        0.0000  \n",
       "3              neutro          negative        0.0000  \n",
       "5              neutro          negative        0.0000  \n",
       "...               ...               ...           ...  \n",
       "22500          neutro          negative        0.5106  \n",
       "22501          neutro          negative        0.0000  \n",
       "22502          neutro          negative        0.0000  \n",
       "22503          neutro          negative        0.0000  \n",
       "22504          neutro          negative        0.0000  \n",
       "\n",
       "[21357 rows x 9 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2852de53f51e57db0fb3c92b7d03cd28fd1e6acc643c7bf23f4addc7db48be7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
